{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "directed-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "\n",
    "from utils.main_utils import get_readers, collect_paths_with_meta\n",
    "from utils.scoring import calc_scores\n",
    "from utils.generate_mcc_images import generate_mcc_images\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Conv2d, ReLU, BatchNorm2d, MaxPool2d, Linear, Dropout\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "instrumental-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCDataset(Dataset):\n",
    "    '''\n",
    "    Класс датасета. Концепция заключается в том, что хранилище изображений \n",
    "    одинаково для всех классов, разделение на тренировочный, валидацинный и тестовый \n",
    "    наборы происходит по списку названий файлов\n",
    "    '''\n",
    "    def __init__(self, path, transforms, file_names):\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(file_names)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.imgs[idx]\n",
    "        target = int(img_name.split('_')[0] == 'F')\n",
    "        img_path = join(self.path, img_name)\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adequate-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''\n",
    "    Класс сверточной нейронной сети. Батч-нормализация давала нестабильное обучение, \n",
    "    поэтому не используется\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = Sequential(\n",
    "            Conv2d(1, 6, 5), ReLU(), MaxPool2d(2,2),\n",
    "            Conv2d(6, 16, 5), ReLU(), MaxPool2d(2,2),\n",
    "            Conv2d(16, 32, 3), ReLU(), MaxPool2d(2,2),\n",
    "            Conv2d(32, 64, 3), ReLU(), MaxPool2d(2,2)  \n",
    "        )\n",
    "        \n",
    "        self.classifier = Sequential(\n",
    "            Linear(38016, 2048), ReLU(), Dropout(0.5),\n",
    "            Linear(2048, 256), ReLU(), Dropout(0.5),\n",
    "            Linear(256, 64), ReLU(), Dropout(0.5),\n",
    "            Linear(64, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(self.feature_extractor(x), 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dutch-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valloader):\n",
    "    preds, y = [], []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs).detach().cpu()\n",
    "            preds.extend(np.argmax(outputs,axis=1).tolist())\n",
    "            y.extend(labels.tolist())\n",
    "    model.train()\n",
    "    return calc_scores(np.array(y), np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "killing-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'data/dev-clean/'\n",
    "SEED = 42\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "mcc_images_path = 'mcc_images/'\n",
    "\n",
    "split_ratio = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sufficient-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "weighted-carpet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reader</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1919</td>\n",
       "      <td>F</td>\n",
       "      <td>data/dev-clean/1919/142785/1919_142785_000005_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1919</td>\n",
       "      <td>F</td>\n",
       "      <td>data/dev-clean/1919/142785/1919_142785_000118_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1919</td>\n",
       "      <td>F</td>\n",
       "      <td>data/dev-clean/1919/142785/1919_142785_000035_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1919</td>\n",
       "      <td>F</td>\n",
       "      <td>data/dev-clean/1919/142785/1919_142785_000064_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1919</td>\n",
       "      <td>F</td>\n",
       "      <td>data/dev-clean/1919/142785/1919_142785_000071_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reader gender                                               path\n",
       "0    1919      F  data/dev-clean/1919/142785/1919_142785_000005_...\n",
       "1    1919      F  data/dev-clean/1919/142785/1919_142785_000118_...\n",
       "2    1919      F  data/dev-clean/1919/142785/1919_142785_000035_...\n",
       "3    1919      F  data/dev-clean/1919/142785/1919_142785_000064_...\n",
       "4    1919      F  data/dev-clean/1919/142785/1919_142785_000071_..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получим список дикторов с путями до их записей\n",
    "readers = get_readers('data/speakers.tsv', audio_path)\n",
    "meta_paths = collect_paths_with_meta(audio_path, readers)\n",
    "meta_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hollow-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# предпосчитаем и сохраним спектрограммы, если они еще не вычисленны\n",
    "if not os.path.exists(mcc_images_path):\n",
    "    generate_mcc_images(mcc_images_path, meta_paths, n_jobs=12, batch_size=100, hop_length=HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pleasant-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conscious-iceland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 8, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вычислим по сколько дикторов будет приходится на каждую из подгрупп данных \n",
    "all_readers = np.unique(meta_paths.reader)\n",
    "readers_number = all_readers.shape[0]\n",
    "train_size = int(readers_number*split_ratio)\n",
    "val_size = (readers_number - train_size)//2\n",
    "test_size = readers_number - train_size - val_size\n",
    "train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "authorized-sunset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3312, 1322, 1102)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Извлечем названия изображений каждой из подгрупп данных\n",
    "train_readers = set(np.random.choice(all_readers, train_size, replace=False).tolist())\n",
    "val_readers = set(np.random.choice(list(set(all_readers)-train_readers), val_size, replace=False).tolist())\n",
    "test_readers = set(all_readers)-train_readers-val_readers\n",
    "\n",
    "train_paths = meta_paths[[el in train_readers for el in meta_paths.reader]][['gender', 'path']]\n",
    "train_paths = [f'{row.gender}_{os.path.basename(row.path).split(\".\")[0]}.png' for _, row in train_paths.iterrows()]\n",
    "\n",
    "val_paths = meta_paths[[el in val_readers for el in meta_paths.reader]][['gender', 'path']]\n",
    "val_paths = [f'{row.gender}_{os.path.basename(row.path).split(\".\")[0]}.png' for _, row in val_paths.iterrows()]\n",
    "\n",
    "test_paths = meta_paths[[el in test_readers for el in meta_paths.reader]][['gender', 'path']]\n",
    "test_paths = [f'{row.gender}_{os.path.basename(row.path).split(\".\")[0]}.png' for _, row in test_paths.iterrows()]\n",
    "\n",
    "len(train_paths), len(val_paths), len(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "greater-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose(\n",
    "    [\n",
    "     transforms.Resize(size=(225,225*4)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = MCCDataset(mcc_images_path, trans, train_paths)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "valset = MCCDataset(mcc_images_path, trans, val_paths)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = MCCDataset(mcc_images_path, trans, test_paths)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "greek-macedonia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose_index = len(trainloader) // 5 \n",
    "verbose_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "understanding-parameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch/batch: 0/10 \n",
      "\t train loss: 0.6980705440044404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/venvs/main/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t {'accuracy': 0.716, 'f1': 0.834, 'matthews': 0.0}\n",
      "epoch/batch: 0/20 \n",
      "\t train loss: 0.6885713100433349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/venvs/main/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t {'accuracy': 0.716, 'f1': 0.834, 'matthews': 0.0}\n",
      "epoch/batch: 0/30 \n",
      "\t train loss: 0.6978034198284149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/venvs/main/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t {'accuracy': 0.716, 'f1': 0.834, 'matthews': 0.0}\n",
      "epoch/batch: 0/40 \n",
      "\t train loss: 0.6899414002895355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/venvs/main/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t {'accuracy': 0.716, 'f1': 0.834, 'matthews': 0.0}\n",
      "epoch/batch: 0/50 \n",
      "\t train loss: 0.6719423770904541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/venvs/main/lib/python3.7/site-packages/sklearn/metrics/_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t {'accuracy': 0.716, 'f1': 0.834, 'matthews': 0.0}\n",
      "epoch/batch: 1/10 \n",
      "\t train loss: 0.6257011234760285\n",
      "\t {'accuracy': 0.844, 'f1': 0.888, 'matthews': 0.636}\n",
      "epoch/batch: 1/20 \n",
      "\t train loss: 0.5208333224058151\n",
      "\t {'accuracy': 0.89, 'f1': 0.923, 'matthews': 0.73}\n",
      "epoch/batch: 1/30 \n",
      "\t train loss: 0.3992552518844604\n",
      "\t {'accuracy': 0.743, 'f1': 0.781, 'matthews': 0.58}\n",
      "epoch/batch: 1/40 \n",
      "\t train loss: 0.32886877954006194\n",
      "\t {'accuracy': 0.782, 'f1': 0.82, 'matthews': 0.628}\n",
      "epoch/batch: 1/50 \n",
      "\t train loss: 0.35269259810447695\n",
      "\t {'accuracy': 0.87, 'f1': 0.9, 'matthews': 0.748}\n",
      "epoch/batch: 2/10 \n",
      "\t train loss: 0.35853367000818254\n",
      "\t {'accuracy': 0.87, 'f1': 0.9, 'matthews': 0.749}\n",
      "epoch/batch: 2/20 \n",
      "\t train loss: 0.2792656645178795\n",
      "\t {'accuracy': 0.852, 'f1': 0.885, 'matthews': 0.723}\n",
      "epoch/batch: 2/30 \n",
      "\t train loss: 0.2512901671230793\n",
      "\t {'accuracy': 0.677, 'f1': 0.709, 'matthews': 0.507}\n",
      "epoch/batch: 2/40 \n",
      "\t train loss: 0.3354091927409172\n",
      "\t {'accuracy': 0.844, 'f1': 0.878, 'matthews': 0.711}\n"
     ]
    }
   ],
   "source": [
    "early_stop = [0, -1, False]\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for batch_ind, (inputs, labels) in enumerate(trainloader, 1):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if not (batch_ind % verbose_index):   \n",
    "            print(f'epoch/batch: {epoch}/{batch_ind} \\n\\t train loss: {running_loss/verbose_index}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            scores = evaluate(model, valloader)\n",
    "            print('\\t', scores)\n",
    "            tracking_score = scores['matthews']\n",
    "            if early_stop[1] >= tracking_score:\n",
    "                early_stop[0] += 1\n",
    "            else:\n",
    "                early_stop = [0, tracking_score, False]\n",
    "            \n",
    "            if epoch > 1 and early_stop[0] >= 3:\n",
    "                early_stop[2] = True\n",
    "                break\n",
    "            \n",
    "    if early_stop[2]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "large-intermediate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.968, 'f1': 0.93, 'matthews': 0.909}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на метрики на тестовом наборе\n",
    "evaluate(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "returning-large",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.91, 'f1': 0.919, 'matthews': 0.82}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на метрики на тренировочном наборе\n",
    "evaluate(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "engaged-doubt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.844, 'f1': 0.878, 'matthews': 0.711}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на метрики на валидационном наборе\n",
    "evaluate(model, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "!Z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
